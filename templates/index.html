<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Sentiment Analyzer</title>

  <!-- Bootstrap 5 -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- TensorFlow + Toxicity -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.12.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/toxicity"></script>

  <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body class="bg-light">

  <div class="container mt-5">
    <div class="card shadow-sm">
      <div class="card-body text-center">
        <h1 class="mb-2">Sentiment Analyzer</h1>
        <p class="text-muted" style="text-align: left;">Detects text that contains toxic content such as threatening language, insults, obscenities, identity-based hate, or sexually explicit language</p>

        <!-- Message Input -->
        <textarea id="messageInput" class="form-control mb-2" rows="4" placeholder="Type your message here..."></textarea>

        <!-- Analyze Button -->
        <button id="analyzeBtn" class="btn btn-primary" disabled onclick="analyzeMessage()">Analyze</button>

        <!-- Loading Spinner -->
        <div id="loadingSpinner" class="mt-3">
          <div class="spinner-border text-primary" role="status"></div>
          <p class="mt-2 mb-0 text-muted">Loading model...</p>
        </div>

        <!-- Result -->
        <div id="result" class="alert d-none mt-4">

          <p class="mb-0" id="verdict"></p>

        </div>
      </div>
    </div>


<details class="mt-3">
  <summary class="text-primary fw-semibold" style="cursor: pointer;">
    Tips for Accuracy
  </summary>
  <div class="mt-2 text-muted small text-start">
    <ul class="mb-0">
      <li>Keep input to <strong>16 words or fewer</strong> for best performance.</li>
      <li>The model works best with <strong>blunt or obvious language</strong>.</li>
      <li><strong>Neutral or sarcastic</strong> messages may not trigger labels.</li>
      <li>Subtle emotional tone is often missed — this model focuses on direct toxicity, not full sentiment analysis.</li>
      <li>This is a <strong>lightweight model that runs entirely in your browser</strong>, so results are fast and private — but not as deep as server-side models.</li>
    </ul>
  </div>
</details>


  </div>

  <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>
